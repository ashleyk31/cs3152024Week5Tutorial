{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589c1e17",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Tutorial for the Naive Bayes classifier using scikit-learn. This example uses Pyktok data to classify TikTok videos as ads or non-ads.\n",
    "\n",
    "Code based on tutorial from StackAbuse: https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/  \n",
    "\n",
    "### 1. Preparing our data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3039cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/lh50ntf51kvdjr8cmpd9y7840000gn/T/ipykernel_14265/1404863864.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5de966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pyktok_ad_data.csv',\n",
    "                   usecols=['video_id', 'suggested_words', 'video_description', 'video_is_ad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360685f",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbffc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       cat tunnel bed, cat brush, cattok, pet products, cats of tiktok, tik tok shop, tiktok shop items, powder donut face pets, cat accessories, toothbrush cat\n",
       "1        cider sweater dress, sweater dress, knitted sweater dress, knitted dress, cider dress, 2 piece knitted dress, dresses, viral dress tiktok, dress outfit, fashion dresses\n",
       "2                        homeika pet vacuum, dog grooming vacuum, tik tok shop, homeika vacuum, homeika, dog grooming, pet vacuum, belgian malinois, vacuums, vacuum cleaner pets\n",
       "3    cat water fountain, stainless steel water fountain, cat fountain, stainless steel fountain, uah pet fountain, cattok, fountain, cat of tiktok, stainless steel, pet products\n",
       "4                             lenovo gm2pro earbuds, lenovo earbuds, tiktok shop, lenovo, headphones, gm2 pro, airpods, bluetooth earbuds, bluetooth earphones, lenovo headphones\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_is_ad'] = df.video_is_ad.map({False: 0, True: 1})\n",
    "\n",
    "#merge the suggested_words and video_description columns\n",
    "df['description'] = df['suggested_words'].combine_first(df['video_description'])\n",
    "\n",
    "#lowercase and remove punctuation\n",
    "df['description'] = df.description.map(lambda x: x.lower())\n",
    "df['description'] = df.description.str.replace('[^\\w\\s]', '')\n",
    "\n",
    "df['description'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc848c1-0e9c-49ca-b370-e049a6c5baa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49754",
   "metadata": {},
   "source": [
    "#### Tokenize the descriptions into separate words using nltk\n",
    "\n",
    "You will need to install the nltk library, if you don't have it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e22a47-ee96-4ea0-a915-e74bc7832be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/younakang/miniconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/younakang/miniconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/younakang/miniconda3/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/younakang/miniconda3/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/younakang/miniconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d942be",
   "metadata": {},
   "source": [
    "***NOTE:***\n",
    "The code below will open a dialog window to ask you to downlaod some packages. In that window, switch to the \"Models\" tab and choose \"punkt\" from the \"Identifier\" column. Click \"Download\" and it will install the necessary files to apply tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278a6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8beb2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ee750",
   "metadata": {},
   "source": [
    "#### Perform word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82851282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>suggested_words</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_is_ad</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7305500598084881706</td>\n",
       "      <td>Cat Tunnel Bed, Cat Brush, cattok, pet products, Cats Of TikTok, tik tok shop, TikTok Shop Items, powder donut face pets, cat accessories, toothbrush cat</td>\n",
       "      <td>Diz and Dub bith love their cat donut from the tiktok shop! Get it before sale runs out! üòª #fyp #foryou #petbrush #cattunnel #catcave #TikTokShop  #catmom #catdad #catperson #smurfcat #PetsOfTikTok #catbed #catsoftiktok #petlover #catlover #happycat #save #foryoupage #catdonut #tiktokshopblackfriday #tiktokshopcybermonday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cat, tunnel, bed, ,, cat, brush, ,, cattok, ,, pet, product, ,, cat, of, tiktok, ,, tik, tok, shop, ,, tiktok, shop, item, ,, powder, donut, face, pet, ,, cat, accessori, ,, toothbrush, cat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7303242532790619422</td>\n",
       "      <td>cider sweater dress, Sweater Dress, knitted sweater dress, knitted dress, cider dress, 2 piece knitted dress, dresses, Viral Dress TikTok, Dress Outfit, Fashion Dresses</td>\n",
       "      <td>this dress fits like a glove! so flattering &amp; comfy! love that it is a 2 piece as well ü§ç @Cider #sweaterdress #knitteddress #viraldress #cider #shopcider #TikTokShopBlackFriday #tiktokshopcybermonday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cider, sweater, dress, ,, sweater, dress, ,, knit, sweater, dress, ,, knit, dress, ,, cider, dress, ,, 2, piec, knit, dress, ,, dress, ,, viral, dress, tiktok, ,, dress, outfit, ,, fashion, dress]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7285271361201687839</td>\n",
       "      <td>homeika pet vacuum, dog grooming vacuum, tik tok shop, homeika vacuum, homeika, dog grooming, Pet Vacuum, Belgian Malinois, vacuums, Vacuum Cleaner Pets</td>\n",
       "      <td>Paw Patrol approves this message. üê∂üöî RUN DONT WALK! @Homeika_Official, I love you. #obsessed #homeikaofficial #homeikavacuum #homeika #tiktokmademebuyit #petgrooming #k9softiktok #k9officer #k9handler #tiktokshop #shoporflop #pawpatrol #holidaysteals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[homeika, pet, vacuum, ,, dog, groom, vacuum, ,, tik, tok, shop, ,, homeika, vacuum, ,, homeika, ,, dog, groom, ,, pet, vacuum, ,, belgian, malinoi, ,, vacuum, ,, vacuum, cleaner, pet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7301040085624016170</td>\n",
       "      <td>Cat Water Fountain, stainless steel water fountain, Cat Fountain, stainless steel fountain, uah pet fountain, Cattok, Fountain, Cat Of Tiktok, Stainless Steel, pet products</td>\n",
       "      <td>Stainless steel water fountains are my favorite. Use code 'WHITEBEARD' for 20% off!! #cats #catsoftiktok #pets #petsoftiktok #tabbycats #tortoiseshellcats #catlover #catslover #cutecats #UAHPET #PetFountain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cat, water, fountain, ,, stainless, steel, water, fountain, ,, cat, fountain, ,, stainless, steel, fountain, ,, uah, pet, fountain, ,, cattok, ,, fountain, ,, cat, of, tiktok, ,, stainless, steel, ,, pet, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7310401556791217450</td>\n",
       "      <td>lenovo gm2pro earbuds, lenovo earbuds, TikTok shop, lenovo, Headphones, gm2 pro, airpods, bluetooth earbuds, bluetooth earphones, Lenovo Headphones</td>\n",
       "      <td>Most insane deal I‚Äôve ever seen #lenovo #gm2pro #lenovogm2pro #earbuds #bluetooth #music #bluetoothearphone #coolgadget #tiktokshopholidaysale #thinkplus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lenovo, gm2pro, earbud, ,, lenovo, earbud, ,, tiktok, shop, ,, lenovo, ,, headphon, ,, gm2, pro, ,, airpod, ,, bluetooth, earbud, ,, bluetooth, earphon, ,, lenovo, headphon]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              video_id  \\\n",
       "0  7305500598084881706   \n",
       "1  7303242532790619422   \n",
       "2  7285271361201687839   \n",
       "3  7301040085624016170   \n",
       "4  7310401556791217450   \n",
       "\n",
       "                                                                                                                                                                suggested_words  \\\n",
       "0                     Cat Tunnel Bed, Cat Brush, cattok, pet products, Cats Of TikTok, tik tok shop, TikTok Shop Items, powder donut face pets, cat accessories, toothbrush cat   \n",
       "1      cider sweater dress, Sweater Dress, knitted sweater dress, knitted dress, cider dress, 2 piece knitted dress, dresses, Viral Dress TikTok, Dress Outfit, Fashion Dresses   \n",
       "2                      homeika pet vacuum, dog grooming vacuum, tik tok shop, homeika vacuum, homeika, dog grooming, Pet Vacuum, Belgian Malinois, vacuums, Vacuum Cleaner Pets   \n",
       "3  Cat Water Fountain, stainless steel water fountain, Cat Fountain, stainless steel fountain, uah pet fountain, Cattok, Fountain, Cat Of Tiktok, Stainless Steel, pet products   \n",
       "4                           lenovo gm2pro earbuds, lenovo earbuds, TikTok shop, lenovo, Headphones, gm2 pro, airpods, bluetooth earbuds, bluetooth earphones, Lenovo Headphones   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     video_description  \\\n",
       "0  Diz and Dub bith love their cat donut from the tiktok shop! Get it before sale runs out! üòª #fyp #foryou #petbrush #cattunnel #catcave #TikTokShop  #catmom #catdad #catperson #smurfcat #PetsOfTikTok #catbed #catsoftiktok #petlover #catlover #happycat #save #foryoupage #catdonut #tiktokshopblackfriday #tiktokshopcybermonday   \n",
       "1                                                                                                                              this dress fits like a glove! so flattering & comfy! love that it is a 2 piece as well ü§ç @Cider #sweaterdress #knitteddress #viraldress #cider #shopcider #TikTokShopBlackFriday #tiktokshopcybermonday   \n",
       "2                                                                           Paw Patrol approves this message. üê∂üöî RUN DONT WALK! @Homeika_Official, I love you. #obsessed #homeikaofficial #homeikavacuum #homeika #tiktokmademebuyit #petgrooming #k9softiktok #k9officer #k9handler #tiktokshop #shoporflop #pawpatrol #holidaysteals   \n",
       "3                                                                                                                       Stainless steel water fountains are my favorite. Use code 'WHITEBEARD' for 20% off!! #cats #catsoftiktok #pets #petsoftiktok #tabbycats #tortoiseshellcats #catlover #catslover #cutecats #UAHPET #PetFountain   \n",
       "4                                                                                                                                                                            Most insane deal I‚Äôve ever seen #lenovo #gm2pro #lenovogm2pro #earbuds #bluetooth #music #bluetoothearphone #coolgadget #tiktokshopholidaysale #thinkplus   \n",
       "\n",
       "   video_is_ad  \\\n",
       "0          NaN   \n",
       "1          NaN   \n",
       "2          NaN   \n",
       "3          NaN   \n",
       "4          NaN   \n",
       "\n",
       "                                                                                                                                                                                                              description  \n",
       "0                         [cat, tunnel, bed, ,, cat, brush, ,, cattok, ,, pet, product, ,, cat, of, tiktok, ,, tik, tok, shop, ,, tiktok, shop, item, ,, powder, donut, face, pet, ,, cat, accessori, ,, toothbrush, cat]  \n",
       "1                   [cider, sweater, dress, ,, sweater, dress, ,, knit, sweater, dress, ,, knit, dress, ,, cider, dress, ,, 2, piec, knit, dress, ,, dress, ,, viral, dress, tiktok, ,, dress, outfit, ,, fashion, dress]  \n",
       "2                                [homeika, pet, vacuum, ,, dog, groom, vacuum, ,, tik, tok, shop, ,, homeika, vacuum, ,, homeika, ,, dog, groom, ,, pet, vacuum, ,, belgian, malinoi, ,, vacuum, ,, vacuum, cleaner, pet]  \n",
       "3  [cat, water, fountain, ,, stainless, steel, water, fountain, ,, cat, fountain, ,, stainless, steel, fountain, ,, uah, pet, fountain, ,, cattok, ,, fountain, ,, cat, of, tiktok, ,, stainless, steel, ,, pet, product]  \n",
       "4                                          [lenovo, gm2pro, earbud, ,, lenovo, earbud, ,, tiktok, shop, ,, lenovo, ,, headphon, ,, gm2, pro, ,, airpod, ,, bluetooth, earbud, ,, bluetooth, earphon, ,, lenovo, headphon]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "df['description'] = df['description'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1575381",
   "metadata": {},
   "source": [
    "#### Use CountVectorizer to transform data into occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09500d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This converts the list of words into space-separated strings\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4570a22",
   "metadata": {},
   "source": [
    "#### Use TF-IDF as model features instead of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce7369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "\n",
    "counts = transformer.transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59564c8d",
   "metadata": {},
   "source": [
    "### 2. Using the Naive Bayes Model\n",
    "\n",
    "#### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dc9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "shuffled_df = df.sample(frac=1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, shuffled_df['video_is_ad'], \n",
    "                                                    test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768ccf6",
   "metadata": {},
   "source": [
    "#### Fit the data to a Naive Bayes classifier.\n",
    "\n",
    "We use the Multinomial Naive Bayes Classifier here for text classification. There are other types of Naive Bayes classifiers for a variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee3a09f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m MultinomialNB()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:732\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    733\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    735\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:578\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    577\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39mreset)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1279\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[1;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[0;32m-> 1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1300\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1301\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1302\u001b[0m     _ensure_no_complex_data(y)\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    127\u001b[0m     X,\n\u001b[1;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eddec",
   "metadata": {},
   "source": [
    "#### Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73d36282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(predicted \u001b[38;5;241m==\u001b[39m y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80dc5e0",
   "metadata": {},
   "source": [
    "Our model's accuracy varies between 60-75%, which isn't great...Let's check the number of features and the sparsity of the document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = len(count_vect.get_feature_names_out())\n",
    "print(\"Number of features:\", features)\n",
    "\n",
    "#Sparsity is the number of zero-valued elements divided by the total number of elements\n",
    "sparsity = (1- np.count_nonzero(X_train.toarray()) / np.prod(X_train.shape)) * 100\n",
    "print(\"Sparsity:\", sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca66f65",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to get a better idea of our model's performance:\n",
    "\n",
    "### 3. Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52559fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot confusion matrix\n",
    "ax = sns.heatmap(conf_matrix, annot=True)\n",
    " \n",
    "# set x-axis label and ticks. \n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14, labelpad=20)\n",
    "ax.xaxis.set_ticklabels(['Non-Ad', 'Ad'])\n",
    " \n",
    "# set y-axis label and ticks\n",
    "ax.set_ylabel(\"True label\", fontsize=14, labelpad=20)\n",
    "ax.yaxis.set_ticklabels(['Non-Ad', 'Ad'])\n",
    " \n",
    "# set plot title\n",
    "ax.set_title(\"Confusion Matrix for TikTok Ad Detection Model\", fontsize=14, pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the values for each cell in the confusion matrix:\n",
    "true_neg, false_pos, false_neg, true_pos = conf_matrix.ravel()\n",
    " \n",
    "true_neg, false_pos, false_neg, true_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ca373-6432-4c28-821c-54ffa319761c",
   "metadata": {},
   "source": [
    "**Calculate f1_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bac9a-378e-4d74-b271-e3a17e8a4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test.values, predicted, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
